\chapter{Interactive Segmentation for Textureless Objects}
\label{chapter:Textureless Segmentation}


\section{System Pipeline}
%\subsection{Static Segmentation - Preprocessing}



 We introduced the main algorithms and tools used in this thesis. In this section we will describe one of the main contributions of this work - Interactive Segmentation of Textureless Objects.
 
\begin{figure}[h!]
\centering
  \includegraphics[width=\columnwidth]{figures/segmentation_pipeline.pdf}
  \caption{System pipeline.}
  \label{fig:pipeline}
\end{figure} 
 
 The approach taken to segment textureless objects   
 consists of five main steps as depicted in Figure \ref{fig:pipeline}
  and demonstrated in a video\footnote{\url{http://youtu.be/Bu4LayrGC1s}}.
 In the first step we obtain an RGBD point cloud from the Kinect sensor. In the second step we perform static object
 pre-segmentation which results in a set of categorized object hypotheses $O$,
 with the category being either flat or round, and a list of object parts $P_{o}$ that every object 
 $o \in O$ consists of. Having obtained the object hypotheses $O$, we infer which hypothesis
 is segmented  correctly. For
 that we count the  number of parts that the respective object hypotheses $O$ 
 consists of, and then sample from the Poisson distribution according to the Equation \ref{eq:poisson}.
 After obtaining the probability of the scene being segmented correctly we 
decide if the interactive segmentation algorithm will be used or not. It is worth mentioning that the system is able to perform with a different static segmentation algorithm that can be used as an indicator if we need to use the interactive segmentation algorithm.





\begin{figure*}[t!]  %\renewcommand*{\thesubfigure}{} 
  \begin{centering} 
    \begin{tabular}{p{0.151\textwidth}p{0.171\textwidth}p{0.171\textwidth}p{0.181\textwidth}p{0.171\textwidth}p{0.166\textwidth}}
    \includegraphics[height=1.35cm]{figures/scene1/image_before.jpg}
&    \includegraphics[height=1.35cm]{figures/scene1/pcl_before.png}
&    \includegraphics[height=1.35cm]{figures/scene1/segments.png}
&    \includegraphics[height=1.35cm]{figures/scene1/labels.png}
&    \includegraphics[height=1.35cm]{figures/scene1/pcl_after.png}    
&    \includegraphics[height=1.35cm]{figures/scene1/distances.png}\\

    \includegraphics[height=1.35cm]{figures/scene2/image_before1.jpg}
&    \includegraphics[height=1.35cm]{figures/scene2/pcl_before1.png}
&    \includegraphics[height=1.35cm]{figures/scene2/segments1.png}    
&    \includegraphics[height=1.35cm]{figures/scene2/labels1.png}
&    \includegraphics[height=1.35cm]{figures/scene2/pcl_after1.png}  
&    \includegraphics[height=1.35cm]{figures/scene2/distances.png}

%     \includegraphics[height=1.65cm]{scene3/image_before.jpg}
% &    \includegraphics[height=1.65cm]{scene3/pcl_before.png}
% &    \includegraphics[height=1.65cm]{scene3/segments.png}
% &    \includegraphics[height=1.65cm]{scene3/labels.png}
% &    \includegraphics[height=1.65cm]{scene3/pcl_after.png}    
% &    \includegraphics[height=1.65cm]{scene3/distances.png}
  \end{tabular}
		%\vspace{-2ex}
    \caption[Two test scenes for our pipeline.]{Two test scenes in the top and bottom row respectively. First column:
      original scenes; second column: extracted RGBD features before the
      interaction; third column: parts $P$ from the static segmentation;
      fourth column: object hypotheses $O$ from the static segmentation; fifth column:
      tracked RGBD features after interaction; sixth column: relative
      distances between the tracked features. Plots with the ramp denote distances between features on different objects 
      and plots with the constant values denote distance between features on the same object.}
    \label{fig:scenes}
  \end{centering}
\end{figure*}






 The information obtained from the categorization algorithm is also used in the next step - based on the object category we decide what kind of RGBD feature needs to be extracted and tracked afterwards. Having obtained one of the categories being flat or round, we extract different RGBD features: lines and corners for the flat objects or cylinders and circles for the round ones. As the next step, we look for the best point to push the objects by looking at concave and convex corners in a top-down image of the scene and we execute the arm motion movement at the previously found corner. We move the arm in $1cm$ intervals until we reach a maximum of $5$ 
  pushes. All of the features are tracked during the interaction 
 and the trajectories of feature centroids  are  saved as 6 degrees of freedom transformations. 
 
  
%In sixth  row of  \figref{scenes} the
% relative distances between centroids over the time are depicted. 
In the fourth step, we apply a graph-based algorithm for trajectory clustering that operates on the saved centroids mentioned above. At the end of this step we obtain a set of grouped features where each group of features is considered to belong to the same object. In this way, features that have moved in the same manner are clustered as one feature. 

 In the fifth and last step, the dense model is reconstructed using the region growing algorithm 
where the centroids of tracked and clustered RGBD features are
used as seed points. As an output, we obtain a dense representation of the objects in the scene that may be further used for grasping or object recognition algorithms.

\section{Pre-processing Steps}


\subsection{Static Pre-segmentation of Objects}
\label{sec:static-seg}

In order to determine if the use of the interactive segmentation algorithm is necessary, we need to deploy a static segmentation algorithm that will return the probability of the scene being segmented correctly. With the aid of static segmentation, we can then verify if a scene can be considered as recognized or if a robot has to investigate further in order to segment the objects of interest. Due to our requirement of the likelihood of an object being segmented correctly, we decided to use the classification method presented in \cite{marton12SC} based on part-graph-based hashing.

The basic idea of the classification method we use is that
segmenting objects accurately does not always work
robustly, but over-segmentation is easily realizable ~\cite{soupofsegments,Lai_Fox_2010,mozos11furniture}.
These segments that represent parts of objects can be used
to compute features, and their combinations build up the
objects. If the used feature is additive, the feature that
would be computed for the object is the same as the sum
of the features of its parts.
There are, of course, multiple ways of combining parts,
and not all of them create a valid object. However, one can
test if a combination is correct by checking if the
combined feature vector is known. 

In order to obtain a final object hypothesis, we group the resulting labelled parts with geometric categories together and compare the results with the training data. By using above mentioned method not only do we retrieve the classification result and its probability, but also we gain important information regarding the class of objects we deal with. The further use of this information is explained in section \ref{sec:3dfeatures}.














% As a first step we want to to segment our static scene in flat and round objects.
%In order to achieve a pre-segmentation we make use of the classification method presented 
%in~\cite{marton12SC} based on part-graph-based hashing.
%The basic idea is that segmenting objects accurately 
%in a cluttered scene does not always yield the expected result, as seen in %\figref{scenes} 
%column 4, and can lead to classification failures, 
%but over-segmenting is easily realizable~\cite{soupofsegments,Lai_Fox_2010,mozos11furniture}. 
%We use the classification approach described in~\cite{marton12SC} 
%for categorizing over-segmented object parts in cluttered scenes
%by considering combinations of these parts to compute features
%and classify these efficiently with the aid of hashing.
%The result is a set of labeled parts with geometric categories
%that can be grouped in order to obtain object hypotheses.
%Based on statistics computed from the training data on single objects, 
%we can estimate how likely it is that an object hypothesis is
%correct.

%\textbf{Why using a categorization algorithm for pre-segmentation?}
%In \figref{segmentation_comparison} we have alluded to the fact that it
%is close to impossible to recover the failure cases since the graph-based
%segmentation algorithm as well as the region growing one work on the level
%of pixels and points in the point cloud. Part-graph-based hashing algorithm on
%the other hand is a learning-based algorithm that bears the semantic of the 
%object categories with it. In this particular case we are thus able to collect
%the statistics about the probable number of parts one object is composed of in 
%the training stage and then deduce whether the categorization of a given scene is 
%likely correct or not.

%In the rest of the section we summarize the part-graph-based 
%hashing algorithm briefly and show how we use it to guide
%turn it into a ``rich'' pre-segmentation algorithm that guides
%the interactive segmentation.

\subsubsection{Part Graphs-based Classification}
\label{sec:part-graphs}

This section briefly describes the algorithm behind the above mentioned work \cite{marton12SC}.
For the first step of the Part Graphs-based Classification algorithm, the scene is split into parts based on the low curvature of the points. The result of this step can be seen in Figure \ref{fig:scenes}, column 3. For each of the parts the GRSD- feature(Global Radius-based Surface Descriptor~\cite{irosws11vosch}) is computed. 
 As described in \cite{irosws11vosch}, the authors adapted the GRSD feature to be additive by simplifying it to a simple histogram of neighborhoods of surfaces of different types, neglecting the ray-tracing step. After computing the GRSD features, the authors created a connectivity matrix based on the distance between the previously extracted parts. Authors then created the groups' hash codes by using all possible groupings in the connectivity matrix and the isomorphic graph metrics. Eventually, based on the hash code the classifier is chosen. For a detailed description of this approach 
please refer to the work of Marton et al.~\cite{marton12SC}.
 
The authors trained the classifier on a subset of the dataset from~\cite{lai11db}. Based on that the objects are classified into six categories:  sphere, box, rectangular/flat, cylinder, disk/plate and other.
Having obtained these results from the algorithm we merge categories together such that we end up only with 3 of them: i)boxes and flats being considered as \emph{flat} category ii)cylinders, spheres, disks being considered as \emph{round} and iii)\emph{other}. 


\subsubsection{Verification of Correctness of Segmentation}
\label{sec:probabilities}

Taking into consideration the number of parts described in the previous section, we come up with a method that classifies the objects based on the parts they consist of. Also, if an object was categorized as belonging to a certain category, we can estimate how probable the result of this categorization is. This way, in addition to information of the object category being round or flat we obtain the probability of an object being classified correctly.

In order to specify the average number of parts that a certain category of objects consist of, we present in Figure \ref{fig:poisson} of the number of different object parts, generated in the training stage. Notice that this distribution can be approximate with a Poisson distribution which results in an average error of 1.7\% (and at most roughly 9\%). Equation \ref{eq:poisson} describes how the probability of being an object of a certain category is dependent on the number of parts the object is broken up into. 
In Equation \ref{eq:poisson} $\lambda$ describes the mean of number of parts.

\begin{equation}
\label{eq:poisson}
P(k~parts~forming~a~single~object) = \lambda^k \exp{-\lambda} / k!
\end{equation}

With the aid of the above described probability we can take the classification result from the Part Graph-based Classification algorithm and the number of parts that the object of interest consists of in the current scene into consideration and obtain the probability of the object being classified correctly. With this simple method we use classification probability as an indicator of whether or not the interactive segmentation algorithm should be deployed.  

In order to better explain the algorithm, we refer the reader to an example shown in the first row of Figure \ref{fig:scenes}. In column 4 it can be noticed that the category of the object is flat. We can also notice that there are 6 parts of which the object consists (column 3). Looking up the probability from the Poisson distribution explained before, we see that that the probability of a flat object consisting of 6 parts is below the threshold - $0.3$, thus it is an indicator for a need to use our interactive segmentation algorithm.



\begin{figure}[h!]
\centering
  \includegraphics[width=0.98\columnwidth, trim=0ex 0ex 53ex 0ex, clip]{figures/dataset_stats.pdf}
	\vspace{-2ex}
  \caption{Distribution of number of parts per object and their approximation with a Poisson distribution.}
  \label{fig:poisson}
\end{figure}

\subsection{Push Point Estimation}
\label{sec:push-point}
Once the over or under segmented region of interest has been identified 
according to the above generated distribution, the  appropriate contact
points  between the objects in   the   scene  and   the   robot's   end   
effector is determined. Furthermore, the  direction in which  the robot's  end
effector should move must be chosen.

The contact points where the objects are pushed need to
be such that the push results in different objects moving
differently.
In  this thesis we apply our previously developed approach based on 
the local concavities~\cite{bersch12interactive}. The outcome
of a push depends largely on the arrangement of objects but
intuition suggests (and it is confirmed by our simulation in the next section) that pushing at corners of objects will
result in not only translation, but also rotation of the pushed object. On the other hand, pushing at the
sides of an object may result only in translation and any
neighboring objects can also move together with it as a
rigid body. Object corners are, therefore, good candidates for
contact points for pushes irrespective of the object configuration. Also, since most
commonly  encountered  household   items  have  convex  outlines  when
observed  from  above,  our  system  uses  local
concavities in the  2D contour of an object group  as an indicator for
boundaries between the objects.
We use this heuristic to select our push points and later
validate the accuracy of this heuristic through simulations in
the next section.

\begin{figure}[tb!]
   \begin{center}
     \includegraphics[width=.9\columnwidth]{figures/corners.png}
		\vspace{-2ex}
   \caption[Corners and their directions after morphological operations.]{Corners and their directions after morphological operations. 1st row - image of the scene. 2nd row, 1st column - Corners detected from the top-view image. 2nd row, 2nd column - Directions detected from the top-down image. }
   \label{fig:corners}
 \end{center}
 \end{figure}

In order to detect concave and convex corners we developed the following algorithm. Firstly, we artificially translate and rotate the camera image such that it looks as if it were taken directly from above the scene. From this point an image and a point cloud are taken to determine potential push points for the robot. Having the point cloud information, we use the Plane Segmentation algorithm from the PCL to segment the biggest plane in the scene - a table. We refine the images by using simple morphological operations - opening and closing. After these steps, we obtain black and white images where everything but objects is black, an example is depicted in Figure \ref{fig:corners}. 

Taking this image as an output, we use the Shi-Tomasi corner detector to determine corner positions and their orientation. As the implementation details of the corner-based pushing go beyond the scope of this thesis, we refer the reader to the work of Bersch et al.~\cite{bersch12interactive} for details.

\subsubsection{Push Point Heuristic Validation}
We carried  out several simulations  in physics-based simulator Gazebo\footnote{\url{http://gazebosim.org/}} to validate  our corner
pushing heuristic. To verify that  pushing at corners is  indeed more
effective, we  spawned different  scenes in Gazebo  with two  or three
closely  placed objects. Objects were flat and round, in different orientations, 
and arranged such that they were in solid contact or in single point contact. Solid  contact refers to  both objects being in  contact with
each  other,  whereby  the  contact  area  is  larger  than  a  single
line.
We  then simulated  pushing at  these objects
with a PR2 gripper at many different contact points along the bounding
box of the  objects and in many different  directions. More precisely,
we chose points along the bounding box spaced $2cm$ apart and for every
such  point,  the  gripper simulated  a  sequence  of  2 pushes  in  7
different directions {15\textdegree}  apart (See Figure~\ref{fig:simulations}). The starting gripper pose
and the object  poses before and after every  push were recorded. Then
Shi-Tomasi features with known but randomly determined  locations were spawned artificially on
the objects. Based on the  recorded object poses, the locations of all
the  features were  computed  after  every push.  This  enabled us  to
compute the simulated optical flows. The feature trajectories obtained in this way were
then clustered using our previously developed segmentation for textured objects algorithm~\cite{bersch12interactive}.
We used this algorithm because of the character of the features - there are many different forms of textureless features and it is difficult to develop a general solution whereas Shi-Tomasi features are good representatives for many textured tracking algorithms.  The contact points for the
pushes  which resulted in  a successful  segmentation of  objects were
then  observed  and  the  number  of  successful  corner  pushes  were
counted. A push  was classified as a corner push  if the contact point
is less than $1cm$ away from an object's corner.

\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=0.3\textwidth]{figures/push_directions.pdf}  
    \includegraphics[width=0.3\textwidth]{figures/rviz_empty9_crop.pdf} 
    \caption[Screenshots from  the Gazebo  simulation of a  two object
      scene   and  the   corresponding  visualization   of  the
      segmentation result.]{Screenshots from  the Gazebo  simulation of a  two object
      scene (left)   and  the   corresponding  visualization   of  the
      segmentation result. The black arrows in the left image show the
      7 push  directions for a single  contact point. The  dots on the
      objects in  the right image represent features  and their colors
      represent  the cluster they  were assigned  to for  a particular
      successful push sequence. The red arrows represent the starting gripper positions
      and  directions  of  all  the  successful push  sequences  in  a
      simulation run. }
         \label{fig:simulations}
  \end{center}
\end{figure}

We carried out 5 runs\footnote{Please mind that since Gazebo uses an ODE engine 
which is based on linear complementarity problem constraint formulation and since the
simulation is dependent on the CPU load, the runs are not fully deterministic.} 
on each of the 24 different  scenes (11 scenes  with 2
objects, 13  scenes with  3 objects), which resulted in an average  of 381.5
push sequences for a scene out of which an average of 14.9 pushes were
successful in segmenting  all the objects in the  scene correctly. Out
of these, 7.25  pushes were corner pushes. There was an average  of 10
object corners in each scene. From this it follows that there were on average
70 corner pushes and 311.5 other pushes. This gives the segmentation success rate of $10\%$
for the corner pushes and $2.4\%$ for the non-corner ones. The reason for the low
overall segmentation result compared to the real scenes above is, on one hand, in that the scenes in the simulation
included the single contact points between the objects and on the other hand, various non-favorable orientations per contact point were computed and executed.
We observed  that corner pushing was successful  in all  the scenes, while  side pushing  was successful
only  when  the  objects were in single point contact. 
When the  objects were next to each other  and similar in size,
pushing  at the sides  resulted in  the objects  moving together  as a
single rigid body, thus making the algorithm fail. In such cases, only
corner  pushes succeeded.  These simulations  thus prove the benefits of
corner pushing irrespective of object arrangement.



\section{Textureless Object Segmentation}
\label{sec:textureless}

The main challenge in dealing with textureless objects is finding good and stable features. In our approach, we decided to make use of the information we have from the previous steps, namely the category of the object. Since there is no texture on the object we have to base our features mainly on the depth information. We employ 3D features provided by the PCL, but we seek different features in respective categories. In the flat category it is more likely that an object has sharp edges and is more box-like, therefore we use 3D lines and 3D corners on this category of objects. In the round category it is more likely to have cylindrical objects as a mug or bowl, therefore, we employ 3D circles and 3D cylinders as the features in this case.

The feature extraction part of our algorithm forms an input for the feature tracking part which is a crucial part of the system. Therefore, it is very important to extract stable features that can be tracked over the time of interaction with a robot.




\subsection{RGBD Features}
\label{sec:3dfeatures}
In order to reduce our regions of interest and improve the finding of the feature we look only at those parts of a point cloud with a high curvature value. The 3D line extraction algorithm takes as an input, all the edges extracted from the initial point cloud. As the next step we use the RANSAC algorithm to find the best fit between the line model and the filtered input point cloud. The algorithm proceeds similarly as described in Chapter \ref{chapter:Background} - if the current number of inliers is greater than the number of inliers found in the previous iterations, then the current line coefficients are saved. The algorithm runs until the number of inliers exceeds a certain threshold. Having extracted a set of lines we pad each of them with neighbouring points that are within a radius of $5cm$ in order to form an edge that ideally consists of two planes. We want to use edges rather than thin lines because edges form more distinctive geometrical models, so they are easier to track. After this process we employ one additional check to eliminate edges consisting of more than 2 planes, namely we use tthe Plane Segmentation Algorithm from PCL to determine how many planes an edge consists of. If the number of planes is bigger than 2 then we eliminate the edge from the tracking set.

In addition to 3D lines, we use 3D corners as features on the flat objects category. 3D corners are extracted by using a 3D implementation of the Harris corner detector from PCL. In this case instead of taking pixel values into consideration and forming a Harris matrix, depth information of the points is used. Therefore, we are able to detect 3D corners in the scene that are not necessarily the same as their 2D Harris corner equivalents. After the corner is detected, we subtract the corners that we found from the point cloud and start searching again. We pad found corners with the neighbouring point as well and we use additional plane segmentation check where we check if a padded corner consists of exactly 3 planes.

In the round object category, we look for 3D cylinders and circles because they are much more stable and easier to find than previous features on these kind of objects. We take advantage of PCL implementations of both of these features. The 3D cylinder and the 3D circle extraction algorithms are based on RANSAC and similarly to the 3D line approach they look for the best fit to the respective model. The 3D cylinder extraction algorithm contains additional check, which is whether point normals are perpendicular to the cylinder axis or not. Also, this time we use padding and subtracting of already found parts from the point cloud methods to obtain the final features. 

At the end of this step we have up to 4 sets of different features depending on the categories that were found in the scene. All of the point clouds are padded since it significantly improves the likelihood function of the tracker that is described in the next section. The features are depicted in Figure \ref{fig:scenes}, columns 2 and 5, 1st and 2nd row. 




\subsection{Particle Filtering-based Tracking of RGBD Point Clouds}
\label{sec:tracking}
The tracked features from the previous step are used as reference models for the Particle Filtering-based Tracker that is implemented in PCL\footnote{https://github.com/PointCloudLibrary/pcl/tree/master/tracking}. The tracking algorithm itself consists of various steps. At the beginning, the reference model is passed and multiple hypotheses are generated. In order to generate a probable hypothesis, two different motion models are weighted and used as a prediction of the point position: constant velocity model and constant position model. Based on the ratio between these two models, different particles are spawned where each of the particles represents a hypothesis of the object's position. At the end of this step, a re-sampling technique described in~\cite{Walker} is applied. 

In the next step the similarity between pairs of the data points is computed. Pairs are selected by searching for the nearest neighbours between the reference point cloud and the real time point cloud obtained by the tracker. Similarity between the point pairs ($p_{j},q_{j}$) depends on two factors that describe the surface and the color of the point: i) the euclidean distance between two points ($l_{euclidean}$) and ii) distance in HSV (Hue, Saturation, Value) color space between the two neighbours ($l_{color}$). In order to obtain the likelihood of a given particle, the similarity factors are computed and weighted: $\alpha, \beta = 0.5$ in our case. The likelihood is computed as shown below.

\begin{eqnarray}
  l_{j} = l_{euclidean}(p_{j},q_{j})l_{color}(p_{j},q_{j}) \nonumber \\
  l_{euclidean}(p_{j},q_{j}) = \frac{1}{1+\alpha|p_{j}-q_{j}|^2} \nonumber \\
  l_{color}(p_{j},q_{j}) = \frac{1}{1+\beta|p_{j,hsv}-q_{j,hsv}|^2} 
  \label{eq:likelihood}
\end{eqnarray} 

Having calculated the likelihood for each pair, we sum them up in order to obtain the model weight $w_{i} = \sum\limits_{j}l_{j}$. In the last step of the algorithm the model is normalized based on~\cite{AzadMAD11}. It is worth highlighting that the algorithm runs on multiple CPU's and uses various optimization techniques such as KLD-based 
(Kullback-Leibler Divergence) sampling~\cite{Fox01KLD} which enables the real time performance of the tracker.


\subsection{Feature Clustering}

\label{sec:clustering}
 This part of the thesis describes the method used to cluster the features that move rigidly with respect to each other and it was done in collaboration with Zoltan Csaba Marton.

\begin{algorithm}[htb!]\footnotesize
  \tcc{\footnotesize{number of tracked features $n$ and number of time steps $m$,
		     relative distance variation threshold $d_{threshold}$,
		     max allowed percent of consecutive \emph{breaks} $p_{threshold}$,
		     and the set of positions of each feature $T$}}
  \KwIn{$n$, $m$, $d_{threshold}$, $p_{threshold}$, $T$ = \{$t_1...t_m$\}}

  \tcc{\footnotesize{relative distances at $t_1$}}
  $D_{reference}$ = pairwiseL2($t_1$) \\
  \tcc{\footnotesize{nr of consecutive \emph{breaks} between features}}
  $C_{breaks}$ = zeros($n$,$n$) \\
  \tcc{\footnotesize{relative distances at $t_1$}}
  $T_{breaks}$ = zeros($m$,$n$,$n$) \\
  \tcc{\footnotesize{count number of consecutive \emph{breaks}}}
  \ForEach{$t_i \in T$}
  {
    \tcc{\footnotesize{relative distances at $t_i$}}
    $D_{i}$ = pairwiseL2($t_i$) \\
    \tcc{\footnotesize{deviation of distances}}
    $E_{i}$ = $|D_{i} - D_{reference}|$ \\
    \tcc{\footnotesize{\emph{breaking} feature pairs}}
    $B_{i} = \{ (f_1,f_2) | E_{i}[f_1,f_2] > d_{threshold} \}$ \\
    \ForEach{$(f_1,f_2) \in B_{i}$}
    {
      $C_{breaks}[f_1,f_2]++$ \tcc{\footnotesize{increment counter}}
    }
    \ForEach{$(f_1,f_2) \not \in B_{i}$}
    {
      $C_{breaks}[f_1,f_2] = 0$ \tcc{\footnotesize{reset counter}}
    }
    $T_{breaks}[i]$ = $C_{breaks}$ \tcc{\footnotesize{save counter}}
  }
  \tcc{\footnotesize{maximum percentage of consecutive \emph{breaks}}}
  $M_{breaks}$ = max($T_{breaks}$)/$m$ \\
  \tcc{\footnotesize{final adjacency matrix}}
  $A$ = getConnections($M_{breaks} <= p_{threshold}$) \\
  \tcc{\footnotesize{number of clusters based on Laplacian}}
  $nr_{clusters}$ = nrZeros(eigenValues(diag(degrees($A$)) - $A$)) \\

  \tcc{\footnotesize{get features clustered by connectivity}}
  \KwOut{$F_{clusters}$ = connectedComponents($A$)}
  \caption{Graph-based trajectory clustering algorithm. A \emph{break} between features means that the relative distance between them exceeded the given threshold.}
  \label{alg:cluster_trajectories}
\end{algorithm}

\begin{figure}[tb!]
   \begin{center}
     \includegraphics[width=.9\columnwidth]{figures/distribution.png}
		\vspace{-2ex}
   \caption{Clustering success rate on 17 scenes for different values of $p_{threshold}$ (see legend) as a function of $d_{threshold}$ (in meters).}
   \label{fig:clustering}
 \end{center}
 \end{figure}
Having tracked multiple features where some of them can belong to the same object, we need an algorithm to cluster these features together. One of the requirements that makes this problem non-trivial is that one feature on an object should be enough to generate a hypothesis about this object. We developed a graph-based solution that clusters the features that moved in a similar manner and separates the features that deviated from their initial trajectory for a long time. In the graph every feature is represented by a node. An edge between two features(nodes) symbolizes the features that can be clustered together. An edge is based on the euclidean distance between the features' centroids and the distance between quaternions describing their orientation. At the beginning of the interaction, the graph has full connectivity, but during the actions taken by the robot, some of the edges can be broken - features can move in a different manner.
The full algorithm is presented in Algorithm \ref{alg:cluster_trajectories}.

There are two important parameters used in the above described graph-based clustering algorithm: i) $d_{threshold}$ - maximum number of consecutive violations of the relative distance violations threshold and ii) $p_{threshold}$ - percentage of number of violations to the theoretic maximum number of frames.


 Figure \ref{fig:clustering} 
shows an evaluation of the clustering algorithm on 17 scenes from Figure \ref{fig:evaluation1}.
 The use of $p_{threshold}$ is clearly advantageous, and the method works well for a range of the $p_{threshold}$ and the $d_{threshold}$ parameters. 
 Since too low values for $d_{threshold}$ over-segment the features, values over $1.5cm$ are used, and the possible under-segmentations solved
 by applying the whole method iteratively until all the objects are clearly separated.
 




 

\subsection{Dense Model Reconstruction}
\label{sec:dense_model}
 This part of the thesis describes the method to reconstruct a full model of an object and it was done in collaboration with Ferenc Balint-Benczedi.
\begin{algorithm}[htb!]\footnotesize
   \tcc{\footnotesize{set of features $F_{clusters}$,
		      distance threshold $d_{roi\_thresh}$,
		      angle threshold $eps_{thresh}$,
		      seed queue $sq$,
		      regions list $R$,
		      current region $R_i$,
		      list of processed points $processed$} }
   \KwIn{$F_{clusters}$, $d_{roi\_thresh}$, $eps_{thresh}$}
   \ForEach {$f_i \in F_{clusters}$}
   {
    $p_{s,i} $:= centroid($f_i$)
    $sq.add(p_{s,i})$
    \tcc{\footnotesize{select a seed point and add it to a queue}}
    $processed(p_{s,i})$ = $true$ \\
    $R_i := \{p_{s,i}\}$
    \tcc{\footnotesize{initialize region}}
    \While {$sq.notempty()$}
    {
      $N := \{q_j \| dist(q_j, R_i[c]) < d_{roi\_thresh}\}$\\
      \tcc{\footnotesize{select neighborhood}}
      \ForEach {$q_j \in N$}
      {
	\If {$processed(q_j)$ = $true$} 
	{
	$continue$
	}
	\If {$boundary(q_j)$ = $true$} 
	{
	$stopgrowing =true$\\
	$R_i \leftarrow R_i \cup \{q_j\}$
	$processed(q_j)$ = $true$ \\
	$break$
	}
		\If { deg ($\vec{p_{s,i}q_j} , norm(q_j)$) $>$ $eps_{thresh}$   }
	{
	 $R_i \leftarrow R_i \cup \{q_j\}$\\
	 $processed(q_j)$ = $true$ \\
	}
	\Else {$break$}
      }
      \If{$stopgrowing = false$ $\&\&$ $\forall q_j \in N\ $boundary($q_j$) = false   }
	{
	$sq \leftarrow N$
	}
    }
    {$R \leftarrow R_i$}
   }
  \ForEach {$R_i, R_j \in R $ }
  {
    \If {$f_i f_j \in $ same object }
	{$R_i \leftarrow R_i \cup \{R_j\}$}
  }
  \KwOut{Dense models $R_{j}$}
  \caption{Region growing with normals \& boundaries.}
  \label{alg:region_growing}
\end{algorithm}

Considering the connected features $F_{clusters}$ as being part of the same object, we reconstruct 
the dense model of the object using region growing in normal space, which also makes use of the 
borders found at depth discontinuities, as shown in Algorithm \ref{alg:region_growing}. 
The idea for the region growing 
constraints is based on the segmentation described by Mishra et al.~\cite{asICCV2009}, 
where the authors make use of a predefined fixation point and a border map. Since we already 
know the features that are part of the object, we can easily define a seed point for the
region growing. In order to find the best possible seed point, we separate the connected 
features using euclidean clustering, calculate each of the resulting clusters' centroid, and 
then start growing from these. An important condition of the region growing is the 
assumption that objects are often composed of convex parts~\cite{Pogor}.
Therefore, we make sure that during region growing two points are assigned to the same region $R_{i}$
if the angle $eps_{thresh}$ between the vector connecting them and the points normal is close to obtuse as shown in Figure \ref{fig:dense}
(considering the sensor noise level\footnote{\url{http://www.ros.org/wiki/openni_kinect/kinect_accuracy}}, 89$^\circ$ was used).
Once all region-feature pairs have been identified, we reconstruct the dense model.
Since in the trajectory clustering step we already identified the features that belong to the same object,
having multiple regions for the same object is easily dealt with
by merging those regions for which the corresponding features belong to the same object into dense models $R_{j}$.

\begin{figure}[ht!]
  \centering \includegraphics[width=0.5\columnwidth]{figures/dense.png}
  \vspace{2ex}
  \caption[Dense reconstruction algorithm.]{Dense reconstruction algorithm. The point is considered to be on the object if the angle $eps_{thresh}$ between the vector connecting it to the initial point and the point's normal is close to obtuse. 
  }
  \label{fig:dense}
\end{figure}

\section{Evaluation}
\label{sec:evaluation}
\subsection{Experimental Setup}
The system was evaluated on 17 scenes in different configurations
as illustrated in Figure \ref{fig:evaluation1}. The   scenes  are   
numbered  1-17 and arranged according to the legend shown in Figure \ref{fig:table_scenes}.
Though our system can iteratively
cope with multi-object scenes, we performed the evaluation on two-object
scenes with the  finite number  of scene
configurations that  can occur. These configurations  can be split in  three different
ways: i)  size, ii) shape, and iii)  arrangement.  A scene may
consist  of two  objects  of different  sizes  or the  same size.  The
objects may  be either both  flat or both round  or a combination  of these
two.  They  may  also  occur  in  different  arrangements: i) completely
separated, ii) only  touching, iii) one on  top of the other,  or iv) in solid
contact. Some configurations are
infeasible for our approach.  For example, a flat  object and a
round  object cannot  be of the  same  size and a round object  on top  of
another  round object  cannot be  pushed (one  mug on  top  of another
mug). It is also not possible to  have a round object that is in solid
contact with  another round  object. For this  case, we  consider solid
contact as  being two objects touching  with more than  one line, for
example in scene number 17 where also  the handle of the mug  touches the juice box.  

\begin{figure}[ht!]
  \centering \includegraphics[width=0.8\columnwidth]{figures/table_scenes.png}
  \vspace{-2ex}
  \caption{Legend for the different scene configurations. The scenes are shown in Figure \ref{fig:evaluation1}.
  }
  \label{fig:table_scenes}
\end{figure}

It is  important to  emphasize that the above  devised conventions 
refer to  the scenes after a  push. The scenes  before interaction were
designed such that it is difficult or impossible to segment
them using static segmentation techniques.
 
Average time to segment one scene from Figure \ref{fig:evaluation1} amounted to $12.5s$ 
with the pre-segmentation taking $1.5s$, feature extraction $3.5s$, pushing 
$6s$ (tracking runs at $25fps$ for up to 10 features) and dense model reconstruction $1.5s$.
Apart from tracking, all modules perform linearly with the
number of  features and objects  respectively and can thus  easily be
used for larger and more complex  scenes. For all the scenes, the push
point estimation algorithm was used. The only exception was the 'on
top'  arrangements for which the algorithm does not generalize. 
For this reason, and since the scope of the thesis is on the priors from
the static segmentation and RGBD features for
textureless objects and the final dense model reconstruction, we performed
the experiments by manually inducing motions into the corners of the scenes.
In  our  future work, we  will address  finding a generalized  push point algorithm.
\vspace{-0.5ex}
\subsection{Results}
All  the  experiments were  performed  three  times  for each  of  the
17    scenes.    All    the    results   are    presented    in Table
\ref{tab:chart_result} which  shows the segmentation success rate  for every scene. 
The  corresponding figures for this data  can be found in  Figure \ref{fig:evaluation1}. 
The algorithm was never able to segment the scene number 8 and performed poorly for scenes
6 and 13. In these cases, the contact surface of the two objects is large and the objects are of the same size.
Erroneous reconstruction happens due to a lack of a sufficiently good boundary estimation
near the touching surface, and therefore the region growing does not terminate.
This could be alleviated by integrating texture/color-based segmentation methods,
which we plan to investigate in the future.

It  is  important to  note  that  the  overall
segmentation   was    successful   in   more   than    82\%   of   the
experiments.  Table \ref{tab:result_percentage}  shows that the more
objects differ  and the less in  contact they are,  the more successful
the  segmentation becomes.  Our
algorithm  performs extremely well in the  'on top'  arrangement which  is very
challenging for the static segmentation techniques.

\begin{table}[h!]\scriptsize
%\hspace*{-2.5cm}
\centering 
\begin{tabular}{|c|c| c| c| c| c| c|c|c|c|c|c|c|c|c|c|c|c|}
\hline    Scene   number    &1   &    2    &   3    &4   &5&6&7&    8&
9&10&11&12&13&14&15&16&17\\ \hline  Success rate[\%] & 100  & 100 &100
&100&100&33,3  & 100  &  0  & 100&  100&  66,7& 100&33,3  &100&100&100
&66,7\\\hline
\end{tabular}
\caption{Segmentation results for  all 17 scenes.  For each scene there  were 3 experiments conducted.}
      \label{tab:chart_result}
\end{table}

\begin{table}[h!]\scriptsize
%\hspace*{-2.1cm} 
\begin{tabular}{|m{1cm} |p{1cm}| p{1.2cm}| c| c| c| c|c|p{2.7cm}|c|}
\hline    & \centering all    results   &  \centering  different    size    &   same    size
&flat-flat&round-round&round-flat & \centering apart & in contact (touching or in
solid contact) & on top\\ \hline Success rate[\%] & \centering 82,4 & \centering 93,9 &  61,1
&  79,2 & 80,0 & 91,7 & 100 & \centering 66,7 & 91,7\\\hline
\end{tabular}
\caption{Segmentation success rates of different scene configurations.
}
\label{tab:result_percentage}
\end{table}

\begin{figure}[h!]  %\renewcommand*{\thesubfigure}{}
  %\hspace*{-2.6cm}  
  \centering 

    \begin{tabular}{ccccccc}
      \hline
      1 & 2 & 3 & 4 & 5 & 6 & 7 \\
      \hline
      \hline
    \includegraphics[height=1.5cm]{pictures/11.jpg}&
    \includegraphics[height=1.5cm]{pictures/21.jpg}&
    \includegraphics[height=1.5cm]{pictures/31.jpg}&
    \includegraphics[height=1.5cm]{pictures/41.jpg}&
    \includegraphics[height=1.5cm]{pictures/51.jpg}&
    \includegraphics[height=1.5cm]{pictures/61.jpg}&
    \includegraphics[height=1.5cm]{pictures/71.jpg}\\
    %\includegraphics[height=1.5cm]{pictures/81.jpg}&
    %\includegraphics[height=1.5cm]{pictures/91.jpg}\\ 
    \includegraphics[height=1.5cm]{pictures/12.jpg}&
    \includegraphics[height=1.5cm]{pictures/22.jpg}&
    \includegraphics[height=1.5cm]{pictures/32.jpg}&
    \includegraphics[height=1.5cm]{pictures/42.jpg}&
    \includegraphics[height=1.5cm]{pictures/52.jpg}&
    \includegraphics[height=1.5cm]{pictures/62.jpg}&
    \includegraphics[height=1.5cm]{pictures/72.jpg}\\
    %\includegraphics[height=1.5cm]{pictures/82.jpg}&
    %\includegraphics[height=1.5cm]{pictures/92.jpg}\\ 
    \includegraphics[height=1.5cm]{pictures/13.png}&
    \includegraphics[height=1.5cm]{pictures/23.png}&
    \includegraphics[height=1.5cm]{pictures/33.png}&
    \includegraphics[height=1.5cm]{pictures/43.png}&
    \includegraphics[height=1.5cm]{pictures/53.png}&
    \includegraphics[height=1.5cm]{pictures/63.png}&
    \includegraphics[height=1.5cm]{pictures/73.png}\\
    %\includegraphics[height=1.5cm]{pictures/83.png}&
    %\includegraphics[height=1.5cm]{pictures/93.png}\\
    \end{tabular}
     % \hspace*{-2.5cm}  
    \begin{tabular}{ccccc}
      \hline
      8 & 9 & 10 & 11 & 12 \\
      \hline
      \hline
    %\includegraphics[height=1.5cm]{pictures/71.jpg}&
    \includegraphics[height=1.5cm]{pictures/81.jpg}&
    \includegraphics[height=1.5cm]{pictures/91.jpg}& 
    \includegraphics[height=1.5cm]{pictures/101.jpg}&
    \includegraphics[height=1.5cm]{pictures/111.jpg}&
    \includegraphics[height=1.5cm]{pictures/121.jpg}\\
    %\includegraphics[height=1.5cm]{pictures/131.jpg}&
    %\includegraphics[height=1.5cm]{pictures/141.jpg}&
    %\includegraphics[height=1.5cm]{pictures/151.jpg}&
    %\includegraphics[height=1.5cm]{pictures/161.jpg}&
    %\includegraphics[height=1.5cm]{pictures/171.jpg}\\ 
    %\includegraphics[height=1.5cm]{pictures/72.jpg}&
    \includegraphics[height=1.5cm]{pictures/82.jpg}&
    \includegraphics[height=1.5cm]{pictures/92.jpg}&     
    \includegraphics[height=1.5cm]{pictures/102.jpg}&
    \includegraphics[height=1.5cm]{pictures/112.jpg}&
    \includegraphics[height=1.5cm]{pictures/122.jpg}\\
    %\includegraphics[height=1.5cm]{pictures/132.jpg}&
    %\includegraphics[height=1.5cm]{pictures/142.jpg}&
    %\includegraphics[height=1.5cm]{pictures/152.jpg}&
    %\includegraphics[height=1.5cm]{pictures/162.jpg}&
    %\includegraphics[height=1.5cm]{pictures/172.jpg}\\ 
    %\includegraphics[height=1.5cm]{pictures/73.png}&
    \includegraphics[height=1.5cm]{pictures/83.png}&
    \includegraphics[height=1.5cm]{pictures/93.png}&
    \includegraphics[height=1.5cm]{pictures/103.png}&
    \includegraphics[height=1.5cm]{pictures/113.png}&
    \includegraphics[height=1.5cm]{pictures/123.png}\\
    %\includegraphics[height=1.5cm]{pictures/133.png}&
    %\includegraphics[height=1.5cm]{pictures/143.png}&
    %\includegraphics[height=1.5cm]{pictures/153.png}&
    %\includegraphics[height=1.5cm]{pictures/163.png}&
    %\includegraphics[height=1.5cm]{pictures/173.png}\\
    \end{tabular}
	\begin{tabular}{ccccc}
      \hline
      13 & 14 & 15 & 16 & 17 \\
      \hline
      \hline
    \includegraphics[height=1.5cm]{pictures/131.jpg}&
    \includegraphics[height=1.5cm]{pictures/141.jpg}& 
    \includegraphics[height=1.5cm]{pictures/151.jpg}&
    \includegraphics[height=1.5cm]{pictures/161.jpg}&
    \includegraphics[height=1.5cm]{pictures/171.jpg}\\    
    \includegraphics[height=1.5cm]{pictures/132.jpg}&
    \includegraphics[height=1.5cm]{pictures/142.jpg}&
    \includegraphics[height=1.5cm]{pictures/152.jpg}&
    \includegraphics[height=1.5cm]{pictures/162.jpg}&
    \includegraphics[height=1.5cm]{pictures/172.jpg}\\ 
    \includegraphics[height=1.5cm]{pictures/133.png}&
    \includegraphics[height=1.5cm]{pictures/143.png}&
    \includegraphics[height=1.5cm]{pictures/153.png}&
    \includegraphics[height=1.5cm]{pictures/163.png}&
    \includegraphics[height=1.5cm]{pictures/173.png}\\
    \end{tabular}

    	\caption[Results  of the  segmentation for  17  scenes.]{Results  of the  segmentation for  17  scenes.  1st/4th/7th
          image row:  Image before  the push  for scenes.   2nd/5th/8th:
          Image after the push for scenes.  3rd/6th/9th: Point cloud
          after dense model reconstruction for scenes.}
    \label{fig:evaluation1}
\end{figure}

We refer the  reader to all scenes with
the round objects. 
It can be noted that
the Kinect sensor from the used viewpoint (mounted on the head of the human size PR2 robot) always
captures mugs as two spatially non-connected parts.
In order to robustly merge these two parts using segmentation
algorithms operating on point clouds or images of static scenes, 
model-based segmentation algorithms are required. While that constitutes a feasible solution,
the system presented in this
thesis can easily deal with such scenes without a model by clustering the two
parts of the mug since they move rigidly with respect to each other.

 For  the scene in the bottom row  of Figure \ref{fig:scenes},  we can  observe that
 there is only one feature on the left object.  All   the  clustering  algorithms  trying  to
 explicitly cluster  at least one  pair of features with  the constant
 relative  distance over  time would have failed  in this  case.  Using  the
 graph-based clustering method, we are able to disconnect the two nodes
 of  the  graph  and infer  that  there  is a single feature-object association.



\section{Conclusion}
\label{sec:conclusion}
We have  presented a novel interactive segmentation  system, suitable for
the segmentation of textureless  objects in cluttered tabletop scenes.
Integrated  in the  system are  the static  pre-segmentation  based on
geometrical   categorization,  a  push   point and direction estimation, 
RGBD features  suitable for the tracking of
textureless objects,  the graph-based trajectory  clustering algorithm,
and the  dense model reconstruction.  A rigorous  evaluation of the
system on a  set of 17 scenes showed successful segmentation in $82\%$ of the cases.  
The results show the applicability of our system
for objects of similar colors,  shapes and sizes on predominantly flat
and  round surfaces.  