\chapter{Theoretical Background and Tools}
\label{chapter:Background}
This chapter describes algorithms and tools used in this thesis. It contains sufficient information to understand all the tools that we leveraged in interactive segmentation and recognition.
\section{Used Algorithms}
\subsection{RANSAC}
In order to explain other algorithms and tools in this thesis, a basic understanding of RANSAC algorithm~\cite{ransac} is needed. RANSAC (RANdom SAmple Consensus) is an iterative algorithm that enables us to find parameters of a mathematical model in the set of data containing inliers and more importantly outliers of a given model. Pseudocode for the algorithm is presented in Algorithm \ref{alg:ransac}. 

\begin{algorithm}[htb!]
\While {$iterations < k$}
{
$maybe\_inliers$ := n randomly selected values from data
$maybe\_model$ := model parameters fitted to $maybe\_inliers$
$consensus\_set$ := $maybe\_inliers$

\ForEach {point in data not in $maybe\_inliers$}
{

\If {point fits $maybe\_model$ with an error $< t$}
{
add point to $consensus\_set$
}

}

\If {number of elements in $consensus\_set > d$}
{
(this implies that we may have found a good model,now test how good it is)\\
$this\_model$ := model parameters fitted to all points in $consensus\_set$\\
$this\_error$ := a measure of how well $this\_model$ fits these points\\



\If {$this\_error < best\_error$}
{            (we have found a model which is better than any of the previous ones,
            keep it until a better one is found)\\
            $best\_model$ := $this\_model$\\
            $best\_consensus\_set$ := $consensus\_set$\\
            $best\_error$ := $this\_error$\\
}


}
    increment iterations

}


\caption{RANSAC algorithm. Source:~\cite{ransac}}
  \label{alg:ransac}
\end{algorithm}

The most common example to explain RANSAC is a line fitting task in the 2D point dataset that contains outliers as shown in Figure \ref{fig:ransac}. The algorithm randomly picks two points and creates a line equation that contains both of them. In the next step, the number of inliers is counted. With inliers we denote all the points that lie within a certain threshold away from the line. The algorithm proceeds iteratively - if the current number of inliers is greater than the number of inliers found in the previous iterations then the current model coefficients are saved. There are different implementations of RANSAC where the algorithm runs until a minimum number of inliers is found or until the difference between them becomes relatively small. As the result, one obtains a set of points that are best explained by the given model.

It is worth noting that the RANSAC algorithm is used with many different models. In the case of our system we use RANSAC to find the best fit to a line, a corner, a cylinder and a circle model. 



\begin{figure}
\centering

{\includegraphics[width=0.3\columnwidth]{figures/ransac.png}}

\caption{RANSAC line fitting algorithm on an illustrative dataset.}
\label{fig:ransac}
\end{figure}



\subsection{2D and 3D Features}
%As mentioned in the previous section RANSAC algorithm is the main tool used to extract 3D features. The differences between different geometric features that we use is in the models used in RANSAC. We use a different mathematical representation of a feature, for example, a line or a cylinder, in order to extract it from the point cloud. 3D features are used only in the interactive segmentation of textureless objects part of the system. Our object recognition system employs various detectors and descriptors in the image space to extract and match 2D features.%

In order to understand different 2D features that were used in our system it is crucial to be familiar with the concepts behind detectors, descriptors and matchers. We, as humans, have no problems in detecting distinctive features in the images we see. Moreover, we can easily realize that there are some features in the image that we have seen before. To reproduce this process for an autonomous system we need multiple components since an image is seen by a computer as a simple set of pixel values without any high level meaning. First, the system has to detect points of interest in the image that are distinctive enough to find the same points multiple times. The algorithms that are behind this process are called detectors and their goal is to simply find good features. Having detected the region, the next step is to describe the found features such that there is a good chance to find the same features later on, under different conditions. The task of describing the found points belongs to a descriptor. In the next subsections we will describe a detector-descriptor pair that was used in the object recognition part of this thesis.

\subsubsection{SIFT}
SIFT~\cite{lowe2004distinctive} stands for Scale Invariant Feature Transform and is one of the most popular descriptor-detector pairs. The SIFT features are invariant to scale and rotation as well as invariant to some extent to illumination changes and the viewpoint of the camera.  

The detector is employed in different scales and the algorithm is looking for the highest response in difference of Gaussians which corresponds to the best scale where the feature was found. The algorithm to compute difference of Gaussians is depicted in Figure \ref{fig:dog}. Each level of the shown stack of images corresponds to the initial image convolved with the Gaussian function. Two adjacent images from the left are subtracted in order to obtain the difference of Gaussians. In order to detect the local maxima and minima of difference of Gaussians function, the point of interest is compared to its eight neighbors in the current scale and to the respective pixels in the scale one level above and below. As the next step, D. Lowe et al. employs filters that remove low contrast and edge responses using the Hessian Matrix on difference of Gaussians. After all these steps, the found points are considered as being robust enough to be further used.

Once the feature is found, it is crucial to use the feature descriptor to describe it in a distinctive manner.  The SIFT descriptor solves a rotational invariance problem by calculating orientations of the feature's neighboring pixels and creating a histogram of them. After smoothing operations the peak of the smoothed histogram is assigned as the general orientation of the SIFT feature. Moreover, the feature is afterwards rotated such that it can be matched against a similar feature even if they have different orientations in the original image. Thus, the rotational invariance is considered as one of the properties of SIFT.

\begin{figure}
\centering

{\includegraphics[width=0.8\columnwidth]{figures/dog.png}}

\caption{Computation of Difference of Gaussians which is used to obtain scale invariant features. Source:~\cite{lowe2004distinctive}}
\label{fig:dog}
\end{figure}


\begin{figure}
\centering

{\includegraphics[width=0.8\columnwidth]{figures/sift.png}}

\caption{SIFT descriptor. Source:~\cite{lowe2004distinctive}}
\label{fig:sift}
\end{figure}

In order to make SIFT invariant to remaining variations - illumination and 3D viewpoint, a descriptor of a keypoint is created. As shown in Figure~\ref{fig:sift} the orientation histogram in 8 directions is computed. The process is based on image gradients and regards every pixel surrounding the keypoint. This representation shows a feature that also contains information about its surroundings, which makes it more distinctive under different conditions.


\subsection{Point Cloud Processing}
In this section we will describe some of the algorithms that we used to process the point clouds received from the Kinect sensor. The definition of a point cloud is provided by R. Rusu in~\cite{Rusu_ICRA2011_PCL} and states the following:

A point cloud is a data structure that provides a representation of the 3D geometry visible by the depth sensor. This data structure consists of 3D points that have X, Y and Z component and are sampled from the underlying surface.  

Since the received point clouds are not perfect and need some pre-processing, we make extensive use of the open source Point Cloud Library\footnote{http://pointclouds.org} (PCL). Algorithms that were used in our work are described in the following sections.

\subsubsection{Voxel Grid Filtering}
It is a very common practice to reduce the number of points in the point cloud before further proceedings. The reason for this is that the process becomes faster and one does not require a lot of memory to store the data. One of the methods to downsample a point cloud is voxelized grid approach.

A 3D voxel grid is a set of volume elements over the points in the point cloud. Each of them contains a certain number of points which are approximately represented by their centroid or the center of the cube. By approximating the points in the cube with one point we decrease number of points in the point cloud significantly. 

\subsubsection{Plane Fitting}
The plane fitting algorithm is one of the most popular pre-processing steps in perception algorithms. By using the RANSAC algorithm with the model of a plane, it is possible to select the biggest plane from the current point cloud. Having the indices of the points that form the biggest plane in the point cloud, it is possible to extract the plane itself and eventually, remove it. 

In this work we make an assumption that most of the 
objects are to be found on horizontal surfaces such as counters, tables, etc. With this type of physically plausible environment the plane removal algorithm results in the set of objects which can be processed further by euclidean clustering algorithm. Thus, plane fitting and removal process significantly simplifies the further work on the point cloud.



\subsubsection{Euclidean Clustering}
The objective of the Euclidean Clustering algorithm is to group points into multiple clusters based on the euclidean distance between neighboring points. The algorithm takes each point and checks if there exist any neighboring points within a certain euclidean distance from this point. If so, then these points are added to the same cluster and the procedure continues for each of them. If the found point was already examined, then it is not taken into consideration again. If all the points within the set distance were proceeded, the cluster is considered finished and the algorithm continues with another set of points. The algorithm runs as long as all the points are clustered. 

The Euclidean Clustering algorithm enables simple segmentation of objects in an uncluttered scene which can be further used in many applications.   

\subsection{Particle Filter}
Particle Filter~\cite{Thrun02d} is a method that approximates the posterior probability in a controllable, but partially observable system with the Markovian property. 

The simple idea behind the algorithm is to create a particle for each state in which the model could be and then to calculate the importance weight of the particle based on the measurements. For example, in the robot localization problem particles represent the current pose of the robot and their weights are calculated based on the laser sensor readings. Since the motion model is known, it is possible to predict the next position of each particle based on the motion model. Having established the updated position, the expected sensor readings can be predicted as well, which leads to the new weight assignments. After assigning the weights, re-sampling is employed to obtain a more probable set of particles. The probability of a particle being re-sampled is proportional to its importance weight. It is worth mentioning that there exist other re-sampling techniques such as KLD-based sampling~\cite{Fox01KLD}, which was used in this thesis.  At the end of this process there remain only those particles that most probably indicate the correct state.

In our implementation we use a particle-filtering based tracker in order to track the features in the textureless object segmentation part.      



\section{Used Tools}
\subsection{Kinect}

\begin{figure}
\centering

{\includegraphics[width=0.5\columnwidth]{figures/kinect.jpg}}

\caption{The Kinect sensor. Source: \url{http://www.microsoft.com}}
\label{fig:kinect}
\end{figure}

Kinect\footnote{\url{http://www.xbox.com/en-US/KINECT}} is a 3D sensing device that is produced by Microsoft, with the purpose of being a motion sensing device for the Xbox video game console. It consists of an infra-red emitter and an infra-red receiver as well as a RGB camera. The device receives structured light to compute the depth by virtue of matching of projected and perceived patterns.

Structured light is the process of projecting a known pattern of pixels onto the observed surface. Since the projected pattern deforms differently on various surfaces, it enables  the vision system to establish the depth of the scene.

The Kinect sensor was used throughout this whole project as the input device for capturing RGBD- point cloud  data.

\subsection{PR2}

\begin{figure}
\centering

{\includegraphics[width=0.5\columnwidth]{figures/pr2.jpeg}}

\caption{PR2 robot. Source: \url{http://www.willowgarage.com}}
\label{fig:pr2}
\end{figure}

The PR2 stands for ``personal robot'' and it is an android robotics platform created by Willow Garage - a robotics company based in Menlo Park, CA. The platform has two arms and grippers and an omnidirectional base as shown in Figure \ref{fig:pr2}. Additionally, it contains multiple sensors including Kinect and tactile sensors. 8-core servers provide the robot's computing power and a 1.5TB externally removable hard drive together with 500GB internal hard drive assure the memory space of the robot.  

In our system we used the PR2 as the main robotics platform. We made use of the robot's sensors as well as its actuator by implementing the manipulation strategies on the robot.

\subsection{ROS}
Robot Operating System (ROS)\footnote{\url{http://www.ros.org}} is a framework designed as a meta-operating system for robotics applications. 

There are two main components of ROS: a) standard operating system-like core functions and b) ROS packages that can be created by the open source community. The former component contains various services such as messaging processes and package management whereas the latter is designed for users to contribute and share their applications. 


There are three main levels behind ROS: i)the filesystem level, where the system is split into packages with similar structure, ii)the computation graph level that defines ways of communication between different parts of the system, and iii)the community level which has the goal to enable sharing and exchanging software and other resources within the robotics community. ROS contains multiple libraries for implementation of motion planning, perception or control algorithms.

Our system uses ROS-infrastructure heavily by exploiting its communication system which enables us to communicate synchronously as well as asynchronously between different elements of the code. The whole system is wrapped into multiple ROS packages and is shared with the ROS open source community.




\subsection{Gazebo}
Gazebo\footnote{\url{http://www.gazebosim.org}} is an open source physics simulator that can simulate multiple robots, their sensors, actuators and third party objects. It is also integrated into the ROS infrastructure which makes it easier to transfer high level algorithms  from the simulator to the real robot system and vice versa.  

In our system, Gazebo was used to verify the push point hypothesis with the PR2 robot. It enabled us to test multiple scenarios in a very short time without using an actual robot.


\subsection{PCL}
The Point Cloud Library (PCL)\footnote{\url{http://www.pointclouds.org}} is an open source C++ library that contains tools for point cloud processing. It consists of many algorithms that enable us to filter the data, extract features, reconstruct the surface, register multiple point clouds and object segmentation, to name but a few. The architecture of the library follows object-oriented design patterns so it is modular and easy to use. PCL works on multiple platforms including Linux, MacOS, Windows and Android. 

Throughout our project, PCL was the most frequently used library as it enabled us to implement state-of-the-art 3D processing algorithms very fast and efficiently.  

\subsection{OpenCV}
OpenCV\footnote{\url{http://www.opencv.willowgarage.com}} is an Open Source Computer Vision library that contains over 2500 optimized algorithms for Computer Vision. It is by far one of the most popular open source software packages with 7 million downloads. It consists of multiple interfaces such as C++, C, and Python that supports all the major platforms. The architecture of the library is rather flat meaning that it contains mainly function calls which is much different to the highly object oriented design of PCL.

Many algorithms from our system including SIFT implementation and image morphology operations were taken from the OpenCV Library. 








